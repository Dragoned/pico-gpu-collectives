# [PICO](https://github.com/HLC-Lab/pico) â€” Performance Insights for Collective Operations

[![GitHub stars](https://img.shields.io/github/stars/HLC-Lab/pico?style=social)](https://github.com/HLC-Lab/pico/stargazers)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](https://github.com/HLC-Lab/pico/issues)

> ğŸ’« If you find **PICO** useful for your research or benchmarking work, please consider giving it a â­ on [GitHub](https://github.com/HLC-Lab/pico)!

---

**PICO** is a **lightweight**, **extensible**, and **reproducible** benchmarking suite for evaluating and tuning **collective communication operations** across diverse libraries and hardware platforms.

Built for researchers, developers, and system administrators, PICO streamlines the **entire benchmarking workflow**â€”from configuration to execution, tracing, and analysisâ€”across MPI, NCCL, and user-defined collectives.

## â­ Highlights
- ğŸ“¦ **Unified** micro-benchmarking of both CPU and GPU collectives, across a variety of MPI libraries (Open MPI, MPICH, Cray MPICH), NCCL and user-defined  algorithms.
- ğŸ›ï¸ **Guided** configuration via a fully fledged Textual TUI or CLI-driven JSON/flag workflow with per-site presets.
- ğŸ“‹ **Reproducible** runs through environment capture, metadata logging, and timestamped result directories.
- ğŸ§© Built-in **correctness checks** for custom collectives and automatic ground-truth validation.
- ğŸ§­ **Per-phase instrumentation**, going beyond micro-benchmarking, hence the name PICO
- ğŸ§µ Queue-friendly orchestration that compiles, ships, and archives jobs seamlessly on **SLURM clusters** or in local mode for debugging.
- ğŸ“Š **Bundled plotting, tracing, and scheduling utilities** for streamlined post-processing and algorithm engineering.

## Architecture at a Glance

```
ğŸ“ Configuration
 â”œâ”€ ğŸ§© Sources: Textual TUI â€¢ JSON â€¢ CLI flags
 â””â”€ âš™ï¸ Validation & module loading via submit_wrapper.sh

ğŸš€ Orchestration
 â”œâ”€ ğŸ§µ scripts/orchestrator.sh iterates over:
 â”‚    â€¢ Libraries Ã— Collectives Ã— Message Sizes
 â””â”€ ğŸ—ï¸ Builds binaries and dispatches jobs (SLURM or local)

ğŸ§  Execution
 â”œâ”€ pico_core / libpico executables
 â”œâ”€ âœ… Correctness checks
 â””â”€ ğŸ§­ Optional per-phase instrumentation

ğŸ“Š Results
 â”œâ”€ results/<system>/<timestamp>/
 â”‚    â€¢ CSV metrics
 â”‚    â€¢ Logs
 â”‚    â€¢ Metadata
 â”‚    â€¢ Archives
 â””â”€ Post-processing utilities:
      â€¢ plot/ â€¢ tracer/ â€¢ schedgen/
```

## ğŸš€ Quickstart

The recommended way to use **PICO** is through its **Textual TUI**, which guides you from configuration to job submission.

### âš™ï¸ 1. Configure Your Environment

Ensure you have at least one valid environment definition under `config/environment/`.

A working `local` sample is provided, modify it for your local machine.

For remote clusters, you should mirror one of the existing environment templates and adapt it to your site (a setup wizard to simplify this configuration is on its way!)

### ğŸ§­ 2. Create a virtual env and launch the TUI

Create and activate a Python virtual environment, then install the required dependencies for the **TUI**:
```bash
pip install textual rich packaging
```

Start the interactive interface follow the four-step wizard: configure environment, select libraries, choose algorithms, and export.

```bash
python tui/main.py
```

### ğŸ§© 3. Generate a Test Description

Within the TUI, define:

* The target collective(s)
* Message sizes and iteration counts
* Backends (MPI / NCCL / custom)
* Instrumentation and validation settings

The TUI will produce a **test descriptor file** encapsulating all these options.

The export lands in `tests/<name>.json` (full configuration) and `tests/<name>.sh` (shell exports).

### ğŸš€ 4. Run the Benchmark

Execute the generated descriptor using the wrapper script, which handles compilation, dispatch, and archival:

```bash
scripts/submit_wrapper.sh -f [path_to_test_sh_file]
```

This command will orchestrate the full benchmarking workflow â€” locally or on SLURM clusters â€” using your defined environment.

### ğŸ§° Optional: CLI Workflow (Legacy)

You can still invoke **PICO** directly via the CLI to explore options or run ad-hoc tests. If that is desired, after step 1 do:

```bash
scripts/submit_wrapper.sh --help
```

> âš ï¸ **Note:** The CLI path is currently *partially maintained*; some flags may be deprecated as functionality transitions to the TUI.

Example CLI invocation:
```bash
scripts/submit_wrapper.sh \
  --location leonardo \
  --nodes 8 \
  --ntasks-per-node 32 \
  --collectives allreduce,allgather \
  --types int32,double \
  --sizes 64,1024,65536 \
  --segment-sizes 0 \
  --time 01:00:00 \
  --gpu-awareness no
```
- Provide comma-separated lists for datatypes, message sizes, and segment sizes.
- Use `--gpu-awareness yes` and `--gpu-per-node` to benchmark NCCL or CUDA-aware MPI collectives.
- Pass `--debug yes` for quick validation runs with reduced iterations and debug builds.
- When `--compile-only yes` is set, the script stops after building `bin/pico_core` and its GPU counterpart.

### ğŸ’» Dependencies
- A C/C++ compiler and MPI implementation (Open MPI, MPICH, or Cray MPICH). CUDA-aware MPI or NCCL is optional for GPU runs.
- (Optional) CUDA toolkit and a compatible NCCL build for GPU collectives.
- Python 3.9+ with `pip` for the TUI and analysis utilities.
- SLURM for cluster submissions; local mode is supported for functional testing.
- Basic build tools (`make`) and a Bash-compatible shell.

## ğŸ§  Core Components
- `pico_core/` â€” C benchmarking driver that allocates buffers, times collectives, checks results, and writes output.
- `libpico/` â€” Library of custom collective algorithms and instrumentation helpers, selectable alongside vendor MPI/NCCL paths.
- `scripts/submit_wrapper.sh` â€” Entry point that parses CLI flags or TUI exports, loads site modules, builds binaries, activates Python envs, and launches SLURM or local runs.
- `scripts/orchestrator.sh` â€” Node-side runner that sweeps libraries, algorithm sets, GPU modes, message sizes, and datatypes while invoking metadata capture and optional compression.
- `config/` â€” Declarative environment, library, and algorithm descriptions consumed by the TUI and CLI (modules to load, compiler wrappers, task/GPU limits).
- `tui/` â€” Textual-based UI that guides the user through environment selection, library selection, algorithm mix, and exports the shell/JSON bundle for later submission.
- `plot/` â€” Python package and CLI (`python -m plot â€¦`) that turns CSV summaries into line charts, bar charts, heatmaps, and tables.
- `tracer/` â€” Tools for network-awareness studies (link utilization estimates, cluster job monitoring, scatterplots/boxplots).
- `schedgen/` â€” Adapted SPCL scheduler generator used to derive algorithm schedules from communication traces.
- `results/` â€” Storage for raw outputs, metadata CSVs (per system), and helper scripts such as `generate_metadata.py`.

## ğŸ’¡ What Happens During a Run
1. Environment sourcing loads modules, compiler wrappers, MPI/NCCL paths, and queue defaults from `config/environments/<location>.sh`.
2. The Makefile builds `libpico` first, then `pico_core` (CPU) and optionally `pico_core_cuda` (GPU), honouring debug and instrumentation flags.
3. A Python virtual environment is activated and populated with plotting/tracing dependencies on demand.
4. `scripts/orchestrator.sh` iterates over every selected library, collective, datatype, message size, and GPU mode. For each combination it:
   - Prepares per-collective environment variables and propagates algorithm lists to the workers.
   - Generates metadata entries through `results/generate_metadata.py`, capturing cluster, job, library, GPU, and note fields.
   - Runs `pico_core`, which allocates buffers, initializes randomized inputs (deterministic when debugging), executes warmups, measures iterations, and compares the outcome against vendor MPI results.
   - Optionally enables LibPICO instrumentation tags to time internal algorithm phases.
5. Outputs are written under `results/<location>/<timestamp>/`; in non-debug runs the directory can be tarred and optionally deleted.

## ğŸ“ˆ Results and Analysis
- CSV files follow the `<count>_<algorithm>[_<segment>]_datatype.csv` naming scheme with per-iteration timing, statistics-only, or summarized rows depending on `--output-level`.
- Allocation maps (`alloc_<tasks>.csv`) record rank-to-node placement. GPU runs append `_GPU`.
- SLURM logs reside alongside the CSVs (`slurm_<jobid>.out/.err`) unless in debug mode.
- Metadata is appended to `results/<location>_metadata.csv`, enabling cross-run filtering by timestamp, collective, library version, GPU involvement, and notes.
- Example plotting commands:
```bash
python -m plot summary --summary-file results/leonardo/<timestamp>/summary.csv
python -m plot heatmap --system leonardo --timestamp <timestamp> --collective allreduce
python -m plot boxplot --system lumi --notes "production"
```
- The tracer package (`tracer/trace_communications.py`) estimates traffic on global links for recorded allocations, while `tracer/sinfo` can processes week-long job snapshots from monitored clusters.

## ğŸ§ª Instrumentation and Custom Collectives
- Building with `-DPICO_INSTRUMENT` exposes the `PICO_TAG_BEGIN/END` macros defined in `include/libpico.h`. 
  - These can be inserted into LibPICO collective implementations to record per-phase timings, which are emitted into `_instrument.csv` files. Detailed usage and examples are provided in [libpico/instrument.md](./libpico/instrument.md).
  - Instrumentation is supported for CPU collectives; the macros are transparent when GPU paths are enabled.
- To add new algorithms, implement them in `libpico_<collective>.c`, declare them in `include/libpico.h`, and list them in `config/algorithms/<standard>/<library>/<collective>.json`. The TUI and CLI automatically surface the new options.

## ğŸ§± Extending PICO
- **Environments:** Add new cluster profiles by cloning `config/environment/<env>` JSON descriptors and creating a matching `config/environments/<env>.sh` wrapper that sets modules, compiler wrappers, and queue defaults.
- **Libraries:** Update `<env>_libraries.json` to expose additional MPI/NCCL builds, compiler flags, GPU capabilities, and metadata strings. The TUI reads these files at runtime.

## ğŸ—‚ï¸ Repository Layout
```
pico/
â”œâ”€â”€ include/                # Public LibPICO API and instrumentation macros
â”œâ”€â”€ libpico/                # Custom collective implementations
â”œâ”€â”€ pico_core/              # Benchmark driver and MPI/NCCL glue code
â”œâ”€â”€ config/                 # Environment, library, and algorithm JSON descriptors
â”œâ”€â”€ scripts/                # Submission, orchestration, metadata, and shell helpers
â”œâ”€â”€ tui/                    # Textual UI for configuration authoring
â”œâ”€â”€ plot/                   # Plotting package and CLI
â”œâ”€â”€ tracer/                 # Network tracing and allocation analysis tools
â”œâ”€â”€ schedgen/               # Communication schedule generator (SPCL fork)
â”œâ”€â”€ tests/                  # Sample exported configurations
â””â”€â”€ results/                # Generated data, metadata CSVs, and helper scripts
```

## ğŸªª Credits and License
**PICO** is developed by *Daniele De Sensi* and *Saverio Pasqualoni* at the Department of Computer Science, Sapienza University of Rome. The project is licensed under the **MIT License**.

Schedgen code was originally released by SPCL @ ETH Zurich under the **BSD 4-Clause license**. The version bundled with PICO includes targeted modifications to support its extended scheduling and tracing workflow.

### ğŸ“¬ Contact
- [desensi@di.uniroma1.it](mailto:desensi@di.uniroma1.it)
- [saverio.pasqualoni@kaust.edu.sa](mailto:saverio.pasqualoni@kaust.edu.sa)
